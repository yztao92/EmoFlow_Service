# File: rag/rag_chain.py

from rag.prompts import RAG_PROMPT
from vectorstore.load_vectorstore import get_retriever_by_emotion
from llm.deepseek_wrapper import DeepSeekLLM
from langchain_core.messages import HumanMessage
from langchain_huggingface import HuggingFaceEmbeddings
import numpy as np
import logging
import re


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# âœ… åµŒå…¥æ¨¡å‹ï¼ˆbge-m3 æœ¬åœ°è·¯å¾„ï¼‰
embedding_model = HuggingFaceEmbeddings(
    model_name="/Users/yangzhentao/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/fake123456",
    model_kwargs={"device": "cpu"},  # æ”¹ä¸º "cuda" å¦‚å¯ç”¨
    encode_kwargs={"normalize_embeddings": True}
)

# âœ… DeepSeek LLM å®ä¾‹
_deepseek = DeepSeekLLM()


def chat_with_llm(prompt: str) -> dict:
    """è°ƒç”¨ DeepSeek æ¨¡å‹ç”Ÿæˆå›å¤"""
    response_text = _deepseek._call([
        HumanMessage(role="user", content=prompt)
    ])
    return {"answer": response_text}

def clean_answer(text: str) -> str:
    """
    å»é™¤å›ç­”é¦–å°¾å¯èƒ½å­˜åœ¨çš„æ•´æ®µå¼•å·ï¼ˆåŒ…æ‹¬ä¸­æ–‡åŒå¼•å·/è‹±æ–‡å¼•å·/å•å¼•å·ï¼‰
    """
    text = text.strip()
    # å»é™¤åŒ¹é…å½¢å¼ï¼š"xxx"ã€â€œxxxâ€ã€'xxx'
    if re.fullmatch(r'^["â€œâ€\'].*["â€œâ€\']$', text):
        return text[1:-1].strip()
    return text

def run_rag_chain(
    emotion: str,
    query: str,
    round_index: int,
    state_summary: str,
) -> str:
    """
    RAG ä¸»é€»è¾‘ï¼š
      - ç”¨ query æ£€ç´¢ top-k å†…å®¹
      - è®¡ç®—å¹¶æ‰“å°ç›¸ä¼¼åº¦
      - æ„é€  Prompt å¹¶è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
    """
    k = 5 if round_index == 1 else 3

    retriever = get_retriever_by_emotion(emotion, k=k)
    docs = retriever.invoke(query)

    # ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
    q_vec = np.array(embedding_model.embed_query(query))
    q_norm = np.linalg.norm(q_vec) + 1e-8
    doc_texts = [d.page_content for d in docs]
    d_vecs = np.array(embedding_model.embed_documents(doc_texts))
    d_norms = np.linalg.norm(d_vecs, axis=1) + 1e-8
    sims = (d_vecs @ q_vec) / (d_norms * q_norm)

    # æ—¥å¿—æ‰“å°
    logging.info(f"\nğŸ§  [æ£€ç´¢] æƒ…ç»ª={emotion}, k={k}ï¼Œæ£€ç´¢åˆ°ï¼š")
    for i, (doc, sim) in enumerate(zip(docs, sims), 1):
        snippet = doc.page_content.replace("\n", " ")[:200]
        logging.info(f"â€”â€” æ–‡æ¡£æ®µ {i} ï¼ˆæƒ…ç»ª={doc.metadata.get('emotion')}ï¼Œç›¸ä¼¼åº¦ {sim*100:.1f}%ï¼‰â€”â€” {snippet}â€¦")

    # æ„é€  Prompt
    context = "\n\n".join(
        f"æ‘˜è¦: {doc.page_content}"
        for doc in docs
    )

    prompt = RAG_PROMPT.format(
        emotion=emotion,
        round_index=round_index,
        state_summary=state_summary,
        context=context,
        question=query
    )

    logging.info("\nğŸ’¡ [ä½¿ç”¨ Prompt]---------------------------------------------------")
    logging.info(prompt)
    logging.info("ğŸ’¡ [End Prompt]---------------------------------------------------\n")

    # ç”Ÿæˆå›ç­”
    res = chat_with_llm(prompt)
    raw_answer = res.get("answer", "").strip()
    answer = clean_answer(raw_answer)
    return answer