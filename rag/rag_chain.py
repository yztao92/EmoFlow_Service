# File: rag/rag_chain.py
# åŠŸèƒ½ï¼šRAGæ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ ¸å¿ƒé€»è¾‘
# å®ç°ï¼šç”¨æˆ·æŸ¥è¯¢ â†’ å‘é‡æ£€ç´¢ â†’ çŸ¥è¯†èåˆ â†’ LLMç”Ÿæˆå›å¤

import logging
import time
import numpy as np
import re
from typing import Dict, Any

# å¯¼å…¥åƒé—®å‘é‡åº“ç³»ç»Ÿ
from vectorstore.qwen_vectorstore import get_qwen_vectorstore, set_qwen_embedding_model
from llm.qwen_embedding_factory import get_qwen_embedding_model
from rag.prompt_router import route_prompt_by_emotion
from rag.prompts import PROMPT_MAP
from llm.llm_factory import chat_with_llm

# å»¶è¿Ÿåˆå§‹åŒ–åƒé—®å‘é‡åº“å’Œembeddingæ¨¡å‹
_qwen_vectorstore = None
_qwen_embedding_model = None

def get_qwen_vectorstore():
    """è·å–åƒé—®å‘é‡åº“å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _qwen_vectorstore
    if _qwen_vectorstore is None:
        from vectorstore.qwen_vectorstore import get_qwen_vectorstore as _get_qwen_vectorstore
        _qwen_vectorstore = _get_qwen_vectorstore()
        # ç¡®ä¿embeddingæ¨¡å‹è¢«è®¾ç½®åˆ°å‘é‡åº“
        embedding_model = get_qwen_embedding_model()
        from vectorstore.qwen_vectorstore import set_qwen_embedding_model
        set_qwen_embedding_model(embedding_model)
    return _qwen_vectorstore

def get_qwen_embedding_model():
    """è·å–åƒé—®embeddingæ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
    global _qwen_embedding_model
    if _qwen_embedding_model is None:
        from llm.qwen_embedding_factory import get_qwen_embedding_model as _get_qwen_embedding_model
        _qwen_embedding_model = _get_qwen_embedding_model()
    return _qwen_embedding_model

def chat_with_llm(prompt: str) -> dict:
    """
    è°ƒç”¨LLMç”Ÿæˆå›å¤
    
    å‚æ•°ï¼š
        prompt (str): å®Œæ•´çš„æç¤ºè¯
    
    è¿”å›ï¼š
        dict: åŒ…å«answerå­—æ®µçš„å“åº”å­—å…¸
    """
    # è¿™é‡Œè°ƒç”¨ä½ çš„LLMå·¥å‚å‡½æ•°
    from llm.llm_factory import chat_with_llm as llm_chat
    return llm_chat(prompt)

def clean_answer(text: str) -> str:
    """
    æ¸…ç†LLMå›å¤æ–‡æœ¬
    
    å‚æ•°ï¼š
        text (str): åŸå§‹å›å¤æ–‡æœ¬
    
    è¿”å›ï¼š
        str: æ¸…ç†åçš„å›å¤æ–‡æœ¬
    """
    text = text.strip()  # å»é™¤é¦–å°¾ç©ºç™½
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ˜¯å¦è¢«å¼•å·åŒ…å›´ï¼Œå¦‚æœæ˜¯åˆ™å»é™¤
    if re.fullmatch(r'^["""\'].*["""\']$', text):
        return text[1:-1].strip()
    return text

def run_rag_chain(
    query: str,
    round_index: int,
    state_summary: str,
    emotion: str = "neutral",  # ä¿ç•™emotionå‚æ•°ç”¨äºPromptè·¯ç”±
) -> str:
    """
    RAG ä¸»é€»è¾‘ï¼šå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæµç¨‹
    
    å‚æ•°ï¼š
        query (str): ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢æ–‡æœ¬
        å‚æ•°æ¥æºï¼šmain.pyä¸­ç”¨æˆ·çš„æœ€æ–°è¾“å…¥
        round_index (int): å½“å‰å¯¹è¯è½®æ¬¡
        å‚æ•°æ¥æºï¼šmain.pyä¸­è®¡ç®—çš„ç”¨æˆ·å‘è¨€è½®æ¬¡
        state_summary (str): å¯¹è¯çŠ¶æ€æ‘˜è¦
        å‚æ•°æ¥æºï¼šStateTracker.summary()æ–¹æ³•ç”Ÿæˆ
        emotion (str): ç”¨æˆ·æƒ…ç»ªçŠ¶æ€ï¼ˆä»…ç”¨äºPromptè·¯ç”±ï¼‰
        å‚æ•°æ¥æºï¼šå‰ç«¯ä¼ å…¥çš„æƒ…ç»ªä¿¡æ¯
    
    è¿”å›ï¼š
        str: AIç”Ÿæˆçš„å›å¤æ–‡æœ¬
    
    æµç¨‹ï¼š
        1. Promptè·¯ç”±ï¼ˆåŸºäºæƒ…ç»ªï¼‰
        2. åƒé—®å‘é‡æ£€ç´¢ï¼ˆçº¯ç›¸ä¼¼åº¦ï¼‰
        3. æ„é€ å®Œæ•´Promptå¹¶è°ƒç”¨LLM
        4. æ¸…ç†å’Œè¿”å›å›å¤
    """
    # ==================== 1. Promptè·¯ç”± ====================
    # ä½¿ç”¨ä¼ å…¥çš„æƒ…ç»ªå‚æ•°è¿›è¡ŒPromptè·¯ç”±
    prompt_key = route_prompt_by_emotion(emotion)
    prompt_template = PROMPT_MAP.get(prompt_key, PROMPT_MAP["default"])
    logging.info(f"[Prompt è·¯ç”±] ä½¿ç”¨ prompt_key: {prompt_key}")

    # ==================== 2. åƒé—®å‘é‡æ£€ç´¢ ====================
    # æ ¹æ®å¯¹è¯è½®æ¬¡å†³å®šæ£€ç´¢æ•°é‡ï¼šç¬¬ä¸€è½®æ£€ç´¢æ›´å¤šå†…å®¹ï¼Œåç»­è½®æ¬¡å‡å°‘
    k = 5 if round_index == 1 else 3
    
    # æ‰§è¡Œåƒé—®å‘é‡æ£€ç´¢ï¼ˆçº¯ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œæ— æƒ…ç»ªè¿‡æ»¤ï¼‰
    start_time = time.time()
    search_results = get_qwen_vectorstore().search(query, k=k)
    retrieve_duration = time.time() - start_time
    logging.info(f"â±ï¸ [åƒé—®æ£€ç´¢è€—æ—¶] {retrieve_duration:.2f} ç§’")

    # ==================== 3. æ£€ç´¢ç»“æœå¤„ç†å’Œæ—¥å¿—è®°å½• ====================
    if not search_results:
        logging.warning("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ç»“æœï¼Œä½¿ç”¨é»˜è®¤å›å¤
        context = "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„çŸ¥è¯†æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    else:
        # è®°å½•æ£€ç´¢ç»“æœ
        logging.info(f"\nğŸ§  [åƒé—®æ£€ç´¢] æƒ…ç»ª={emotion}, prompt={prompt_key}, k={k}")
        for i, result in enumerate(search_results, 1):
            similarity = result.get('similarity', 0)
            title = result.get('title', 'æœªçŸ¥æ ‡é¢˜')
            logging.info(f"â€”â€” æ–‡æ¡£ {i} ï¼ˆç›¸ä¼¼åº¦ {similarity*100:.1f}%ï¼‰â€”â€” {title[:50]}â€¦")

        # ==================== 4. æ„é€ ä¸Šä¸‹æ–‡å’ŒPrompt ====================
        # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ„é€ ä¸ºä¸Šä¸‹æ–‡
        context_parts = []
        for result in search_results:
            answer_summary = result.get('answer_summary', '')
            key_point = result.get('key_point', '')
            suggestion = result.get('suggestion', '')
            
            # ç»„åˆæ–‡æ¡£ä¿¡æ¯
            doc_info = f"æ‘˜è¦: {answer_summary}"
            if key_point:
                doc_info += f"\nå…³é”®ç‚¹: {key_point}"
            if suggestion:
                doc_info += f"\nå»ºè®®: {suggestion}"
            
            context_parts.append(doc_info)
        
        context = "\n\n".join(context_parts)

    # ==================== 5. æ„é€ å®Œæ•´Prompt ====================
    # ä½¿ç”¨Promptæ¨¡æ¿æ ¼å¼åŒ–å®Œæ•´æç¤ºè¯
    prompt = prompt_template.format(
        emotion=emotion,  # å½“å‰æƒ…ç»ªï¼ˆç”¨äºPrompté£æ ¼ï¼‰
        round_index=round_index,  # å¯¹è¯è½®æ¬¡
        state_summary=state_summary,  # çŠ¶æ€æ‘˜è¦
        context=context,  # æ£€ç´¢åˆ°çš„çŸ¥è¯†ä¸Šä¸‹æ–‡
        question=query  # ç”¨æˆ·æŸ¥è¯¢
    )

    # è®°å½•ä½¿ç”¨çš„å®Œæ•´Promptï¼ˆç”¨äºè°ƒè¯•ï¼‰
    logging.info("\nğŸ’¡ [ä½¿ç”¨ Prompt]---------------------------------------------------")
    logging.info(prompt)
    logging.info("ğŸ’¡ [End Prompt]---------------------------------------------------\n")

    # ==================== 6. LLMè°ƒç”¨å’Œå›å¤æ¸…ç† ====================
    # è°ƒç”¨LLMç”Ÿæˆå›å¤
    res = chat_with_llm(prompt)
    raw_answer = res.get("answer", "").strip()  # è·å–åŸå§‹å›å¤
    answer = clean_answer(raw_answer)  # æ¸…ç†å›å¤æ ¼å¼
    return answer
