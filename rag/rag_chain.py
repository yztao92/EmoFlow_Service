# File: rag/rag_chain.py
# åŠŸèƒ½ï¼šRAG (Retrieval-Augmented Generation) æ£€ç´¢å¢å¼ºç”Ÿæˆçš„æ ¸å¿ƒå®ç°
# åŒ…å«ï¼šæƒ…ç»ªæ£€æµ‹ã€çŸ¥è¯†æ£€ç´¢ã€Promptè·¯ç”±ã€LLMè°ƒç”¨ç­‰å®Œæ•´æµç¨‹

from rag.prompt_router import route_prompt_by_emotion  # æ ¹æ®æƒ…ç»ªè·¯ç”±åˆ°ä¸åŒé£æ ¼çš„Prompt
from llm.emotion_detector import detect_emotion  # æƒ…ç»ªæ£€æµ‹æ¨¡å—
from rag.prompts import PROMPT_MAP  # ä¸åŒé£æ ¼çš„Promptæ¨¡æ¿æ˜ å°„
from vectorstore.load_vectorstore import get_retriever_by_emotion  # æ ¹æ®æƒ…ç»ªè·å–å‘é‡æ£€ç´¢å™¨
from llm.deepseek_wrapper import DeepSeekLLM  # DeepSeek LLMåŒ…è£…å™¨
from llm.embedding_factory import get_embedding_model  # è·å–embeddingæ¨¡å‹
from langchain_core.messages import HumanMessage  # LangChainæ¶ˆæ¯æ ¼å¼
import numpy as np  # æ•°å€¼è®¡ç®—åº“ï¼Œç”¨äºç›¸ä¼¼åº¦è®¡ç®—
import logging  # æ—¥å¿—è®°å½•
import re  # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºæ–‡æœ¬æ¸…ç†
import time  # æ—¶é—´æ¨¡å—ï¼Œç”¨äºæ€§èƒ½ç›‘æ§

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å…¨å±€æ¨¡å‹å®ä¾‹
embedding_model = get_embedding_model()  # è·å–embeddingæ¨¡å‹å®ä¾‹
_deepseek = DeepSeekLLM()  # åˆå§‹åŒ–DeepSeek LLMå®ä¾‹

def chat_with_llm(prompt: str) -> dict:
    """
    è°ƒç”¨DeepSeek LLMç”Ÿæˆå›å¤
    
    å‚æ•°ï¼š
        prompt (str): è¾“å…¥ç»™LLMçš„å®Œæ•´æç¤ºè¯
        å‚æ•°æ¥æºï¼šrun_rag_chainå‡½æ•°ä¸­æ„é€ çš„Prompt
    
    è¿”å›ï¼š
        dict: åŒ…å«LLMå›å¤çš„å­—å…¸ï¼Œæ ¼å¼ {"answer": "ç”Ÿæˆçš„å›å¤"}
    """
    response_text = _deepseek._call([
        HumanMessage(role="user", content=prompt)  # æ„é€ LangChainæ¶ˆæ¯æ ¼å¼
    ])
    return {"answer": response_text}

def clean_answer(text: str) -> str:
    """
    æ¸…ç†LLMå›å¤ä¸­çš„å¤šä½™å¼•å·å’Œæ ¼å¼
    
    å‚æ•°ï¼š
        text (str): åŸå§‹LLMå›å¤æ–‡æœ¬
        å‚æ•°æ¥æºï¼šchat_with_llmå‡½æ•°è¿”å›çš„åŸå§‹å›å¤
    
    è¿”å›ï¼š
        str: æ¸…ç†åçš„å›å¤æ–‡æœ¬
    """
    text = text.strip()  # å»é™¤é¦–å°¾ç©ºç™½
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ£€æŸ¥æ˜¯å¦è¢«å¼•å·åŒ…å›´ï¼Œå¦‚æœæ˜¯åˆ™å»é™¤
    if re.fullmatch(r'^["""\'].*["""\']$', text):
        return text[1:-1].strip()
    return text

def run_rag_chain(
    query: str,
    round_index: int,
    state_summary: str,
) -> str:
    """
    RAG ä¸»é€»è¾‘ï¼šå®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆæµç¨‹
    
    å‚æ•°ï¼š
        query (str): ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢æ–‡æœ¬
        å‚æ•°æ¥æºï¼šmain.pyä¸­ç”¨æˆ·çš„æœ€æ–°è¾“å…¥
        round_index (int): å½“å‰å¯¹è¯è½®æ¬¡
        å‚æ•°æ¥æºï¼šmain.pyä¸­è®¡ç®—çš„ç”¨æˆ·å‘è¨€è½®æ¬¡
        state_summary (str): å¯¹è¯çŠ¶æ€æ‘˜è¦
        å‚æ•°æ¥æºï¼šStateTracker.summary()æ–¹æ³•ç”Ÿæˆ
    
    è¿”å›ï¼š
        str: AIç”Ÿæˆçš„å›å¤æ–‡æœ¬
    
    æµç¨‹ï¼š
        1. è‡ªåŠ¨æƒ…ç»ªè¯†åˆ« + Promptè·¯ç”±
        2. æ ¹æ®æƒ…ç»ªæ£€ç´¢ç›¸å…³çŸ¥è¯†
        3. æ„é€ å®Œæ•´Promptå¹¶è°ƒç”¨LLM
        4. æ¸…ç†å’Œè¿”å›å›å¤
    """
    # ==================== 1. æƒ…ç»ªè¯†åˆ«å’ŒPromptè·¯ç”± ====================
    # è‡ªåŠ¨è¯†åˆ«ç”¨æˆ·è¾“å…¥çš„æƒ…ç»ª
    emotion = detect_emotion(query)
    # æ ¹æ®æƒ…ç»ªé€‰æ‹©å¯¹åº”çš„Prompté£æ ¼
    prompt_key = route_prompt_by_emotion(emotion)
    # ä»Promptæ˜ å°„ä¸­è·å–å¯¹åº”çš„æ¨¡æ¿
    prompt_template = PROMPT_MAP.get(prompt_key, PROMPT_MAP["default"])
    logging.info(f"[Prompt è·¯ç”±] ä½¿ç”¨ prompt_key: {prompt_key}")  # è®°å½•ä½¿ç”¨çš„Prompté£æ ¼

    # ==================== 2. çŸ¥è¯†æ£€ç´¢ ====================
    # æ ¹æ®å¯¹è¯è½®æ¬¡å†³å®šæ£€ç´¢æ•°é‡ï¼šç¬¬ä¸€è½®æ£€ç´¢æ›´å¤šå†…å®¹ï¼Œåç»­è½®æ¬¡å‡å°‘
    k = 5 if round_index == 1 else 3
    # æ ¹æ®æƒ…ç»ªè·å–å¯¹åº”çš„æ£€ç´¢å™¨ï¼ˆåŒ…å«æƒ…ç»ªè¿‡æ»¤ï¼‰
    retriever = get_retriever_by_emotion(emotion, k=k)

    # æ‰§è¡Œæ£€ç´¢å¹¶è®°å½•è€—æ—¶
    start_time = time.time()
    docs = retriever.invoke(query)  # ä½¿ç”¨LangChainçš„invokeæ–¹æ³•è¿›è¡Œæ£€ç´¢
    retrieve_duration = time.time() - start_time
    logging.info(f"â±ï¸ [æ£€ç´¢è€—æ—¶] {retrieve_duration:.2f} ç§’")

    # ==================== 3. ç›¸ä¼¼åº¦è®¡ç®—å’Œæ—¥å¿—è®°å½• ====================
    # è®¡ç®—æŸ¥è¯¢å‘é‡
    q_vec = np.array(embedding_model.embed_query(query))
    q_norm = np.linalg.norm(q_vec) + 1e-8  # è®¡ç®—æŸ¥è¯¢å‘é‡èŒƒæ•°ï¼ŒåŠ å°å€¼é¿å…é™¤é›¶
    
    # è®¡ç®—æ–‡æ¡£å‘é‡
    doc_texts = [d.page_content for d in docs]  # æå–æ–‡æ¡£å†…å®¹
    d_vecs = np.array(embedding_model.embed_documents(doc_texts))  # æ‰¹é‡è®¡ç®—æ–‡æ¡£å‘é‡
    d_norms = np.linalg.norm(d_vecs, axis=1) + 1e-8  # è®¡ç®—æ–‡æ¡£å‘é‡èŒƒæ•°
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    sims = (d_vecs @ q_vec) / (d_norms * q_norm)

    # è®°å½•æ£€ç´¢ç»“æœå’Œç›¸ä¼¼åº¦
    logging.info(f"\nğŸ§  [æ£€ç´¢] æƒ…ç»ª={emotion}, prompt={prompt_key}, k={k}")
    for i, (doc, sim) in enumerate(zip(docs, sims), 1):
        snippet = doc.page_content.replace("\n", " ")[:200]  # æˆªå–æ–‡æ¡£ç‰‡æ®µç”¨äºæ—¥å¿—
        logging.info(f"â€”â€” æ–‡æ¡£æ®µ {i} ï¼ˆæƒ…ç»ª={doc.metadata.get('emotion')}ï¼Œç›¸ä¼¼åº¦ {sim*100:.1f}%ï¼‰â€”â€” {snippet}â€¦")

    # ==================== 4. æ„é€ ä¸Šä¸‹æ–‡å’ŒPrompt ====================
    # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ„é€ ä¸ºä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ‘˜è¦å’ŒåŸæ–‡
    context = "\n\n".join(
        f"æ‘˜è¦: {doc.page_content}\nåŸæ–‡: {doc.metadata.get('content', '')[:300]}"  # é™åˆ¶åŸæ–‡é•¿åº¦
        for doc in docs
    )

    # ä½¿ç”¨Promptæ¨¡æ¿æ ¼å¼åŒ–å®Œæ•´æç¤ºè¯
    prompt = prompt_template.format(
        emotion=emotion,  # å½“å‰æƒ…ç»ª
        round_index=round_index,  # å¯¹è¯è½®æ¬¡
        state_summary=state_summary,  # çŠ¶æ€æ‘˜è¦
        context=context,  # æ£€ç´¢åˆ°çš„çŸ¥è¯†ä¸Šä¸‹æ–‡
        question=query  # ç”¨æˆ·æŸ¥è¯¢
    )

    # è®°å½•ä½¿ç”¨çš„å®Œæ•´Promptï¼ˆç”¨äºè°ƒè¯•ï¼‰
    logging.info("\nğŸ’¡ [ä½¿ç”¨ Prompt]---------------------------------------------------")
    logging.info(prompt)
    logging.info("ğŸ’¡ [End Prompt]---------------------------------------------------\n")

    # ==================== 5. LLMè°ƒç”¨å’Œå›å¤æ¸…ç† ====================
    # è°ƒç”¨LLMç”Ÿæˆå›å¤
    res = chat_with_llm(prompt)
    raw_answer = res.get("answer", "").strip()  # è·å–åŸå§‹å›å¤
    answer = clean_answer(raw_answer)  # æ¸…ç†å›å¤æ ¼å¼
    return answer
