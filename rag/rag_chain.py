# File: rag/rag_chain.py

from rag.prompts import RAG_PROMPT
from vectorstore.load_vectorstore import get_retriever_by_emotion
from llm.deepseek_wrapper import DeepSeekLLM
from llm.zhipu_embedding import ZhipuEmbedding
from langchain_core.messages import HumanMessage
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(__name__)

# å®ä¾‹åŒ–åµŒå…¥æ¨¡å‹ï¼Œç”¨äºç›¸ä¼¼åº¦è®¡ç®—
_embedding = ZhipuEmbedding()

# å®ä¾‹åŒ– DeepSeek LLM
_deepseek = DeepSeekLLM()

def chat_with_llm(prompt: str) -> dict:
    """
    ä½¿ç”¨ DeepSeek LLM ç”Ÿæˆå›ç­”ï¼Œè¿”å› dict æ ¼å¼
    """
    response_text = _deepseek._call([
        HumanMessage(role="user", content=prompt)
    ])
    return {"answer": response_text}


def run_rag_chain(
    emotion: str,
    query: str,
    round_index: int,
    state_summary: str,
    brief_summary: str,    # æ–°å¢å‚æ•°ï¼Œä¸»çº¿æ‘˜è¦
) -> str:
    """
    åŸºäºæƒ…ç»ªå’Œå¯¹è¯çŠ¶æ€çš„ RAG æµç¨‹ï¼š
      1. æŒ‰ emotion åˆ†åº“æ£€ç´¢ top-k æ–‡æ¡£ï¼ˆç”¨ brief_summary ä½œä¸ºæ£€ç´¢Queryï¼‰
      2. è®¡ç®—å¹¶æ‰“å°ä½™å¼¦ç›¸ä¼¼åº¦
      3. æ„é€ åŒ…å«å¯¹è¯çŠ¶æ€çš„ Prompt
      4. æ‰“å° Prompt å¹¶è°ƒç”¨ LLM

    :param emotion: ç”¨æˆ·å½“å‰æƒ…ç»ªï¼Œå¦‚ 'sad', 'happy', 'tired', 'angry'
    :param query: ç”¨æˆ·æœ€æ–°æé—®
    :param round_index: å¯¹è¯è½®æ¬¡
    :param state_summary: æœ€è¿‘å¯¹è¯å’Œå¹²é¢„çš„çŠ¶æ€æ‘˜è¦ï¼ˆPromptç”¨ï¼‰
    :param brief_summary: LLMç”Ÿæˆçš„ä¸»çº¿æ‘˜è¦ï¼ˆæ£€ç´¢ç”¨ï¼‰
    :return: ç”Ÿæˆçš„å›ç­”
    """
    # åŠ¨æ€è®¾ç½® k: é¦–è½® 5 æ¡ï¼Œåç»­ 3 æ¡
    k = 5 if round_index == 1 else 3

    # 1) ç”¨ brief_summary æ£€ç´¢ï¼ˆå¦‚æ— åˆ™é™çº§ç”¨ queryï¼‰
    retriever = get_retriever_by_emotion(emotion, k=k)
    effective_query = brief_summary.strip() or query.strip()
    docs = retriever.invoke(effective_query)

    # 2) è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ­¤å¤„ç”¨æ£€ç´¢ç”¨çš„ effective_queryï¼‰
    q_vec = np.array(_embedding.embed_query(effective_query))
    q_norm = np.linalg.norm(q_vec) + 1e-8
    doc_texts = [d.page_content for d in docs]
    d_vecs = np.array(_embedding.embed_documents(doc_texts))
    d_norms = np.linalg.norm(d_vecs, axis=1) + 1e-8
    sims = (d_vecs @ q_vec) / (d_norms * q_norm)

    # 3) æ‰“å°æ£€ç´¢æ—¥å¿—
    logging.info(f"\nğŸ§  [æ£€ç´¢] æƒ…ç»ª={emotion}, k={k}ï¼Œæ£€ç´¢åˆ°ï¼š")
    for i, (doc, sim) in enumerate(zip(docs, sims), 1):
        snippet = doc.page_content.replace("\n", " ")[:200]
        logging.info(f"â€”â€” æ–‡æ¡£æ®µ {i} ï¼ˆæƒ…ç»ª={doc.metadata.get('emotion')}ï¼Œç›¸ä¼¼åº¦ {sim*100:.1f}%ï¼‰â€”â€” {snippet}â€¦")

    # 4) æ„é€  Promptï¼Œä»ç„¶æ³¨å…¥ state_summary
    context = "\n\n".join(
        f"æ‘˜è¦: {doc.page_content}\nåŸæ–‡: {doc.metadata.get('content', '')}"
        for doc in docs
    )

    prompt = RAG_PROMPT.format(
        emotion=emotion,
        round_index=round_index,
        state_summary=state_summary,
        context=context,
        question=query
    )

    # 5) æ‰“å°å®é™…ä½¿ç”¨çš„ Prompt
    logging.info("\nğŸ’¡ [ä½¿ç”¨ Prompt]---------------------------------------------------")
    logging.info(prompt)
    logging.info("ğŸ’¡ [End Prompt]---------------------------------------------------\n")

    # 6) è°ƒç”¨ LLM
    res = chat_with_llm(prompt)
    answer = res.get("answer", "").strip().strip('"').strip('"').strip('"')
    return answer