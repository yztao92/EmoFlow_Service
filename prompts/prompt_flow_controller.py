# File: prompts/prompt_flow_controller.py
import logging
from typing import List, Dict, Any
from prompts.chat_prompts_generator import build_final_prompt
from llm.llm_factory import chat_with_llm

try:
    from retriever.search import retrieve
except Exception:
    logging.warning("[prompt_flow] æœªæ‰¾åˆ° retriever.search.retrieveï¼ŒRAG å°†è¢«ç¦ç”¨")
    def retrieve(queries: List[str], top_k: int = 4):
        return []

def chat_once(analysis: dict, state_summary: str, question: str, current_time: str = None, user_id: int = None, user_info: Dict[str, Any] = None, session_id: str = None) -> str:
    # â€”â€” è·å–ç”¨æˆ·è®°å¿†ç‚¹ï¼ˆå¦‚æœæœ‰user_idï¼‰â€”â€” #
    user_memories = []
    if user_id:
        try:
            from memory import get_user_latest_memories
            user_memories = get_user_latest_memories(user_id, limit=10)
            if user_memories:
                logging.info(f"ğŸ“ è·å–åˆ°ç”¨æˆ· {user_id} çš„ {len(user_memories)} ä¸ªè®°å¿†ç‚¹")
            else:
                logging.info(f"ğŸ“ ç”¨æˆ· {user_id} æš‚æ— è®°å¿†ç‚¹")
        except Exception as e:
            logging.warning(f"è·å–ç”¨æˆ·è®°å¿†ç‚¹å¤±è´¥ï¼Œè·³è¿‡ï¼š{e}")
            user_memories = []

    # â€”â€” å¯é€‰ RAG â€”â€” #
    rag_bullets = []
    if analysis.get("need_rag"):
        try:
            docs = retrieve(analysis.get("rag_queries", []), top_k=4)
            rag_bullets = [getattr(d, "snippet", str(d)) for d in (docs or [])]
        except Exception as e:
            logging.warning("RAG æ£€ç´¢å¤±è´¥ï¼Œè·³è¿‡ï¼š%s", e)

    # â€”â€” æ–°å¢ï¼šå®æ—¶æœç´¢RAG â€”â€” #
    if analysis.get("need_live_search"):
        try:
            from llm.qianfan_rag import get_rag_bullets_for_query_with_cache
            live_queries = analysis.get("live_search_queries", [])
            # åªå¤„ç†ç¬¬ä¸€ä¸ªæœç´¢è¯ï¼Œé¿å…å¤šä¸ªæœç´¢æ‹–æ…¢é€Ÿåº¦
            if len(live_queries) > 1:
                live_queries = [live_queries[0]]
            
            for query in live_queries:
                # ä½¿ç”¨å¸¦ç¼“å­˜çš„æœç´¢å‡½æ•°
                if session_id:
                    from llm.search_cache_manager import get_cached_search_result
                    live_bullets = get_rag_bullets_for_query_with_cache(query, session_id)
                else:
                    # å¦‚æœæ²¡æœ‰session_idï¼Œå›é€€åˆ°æ™®é€šæœç´¢
                    from llm.qianfan_rag import get_rag_bullets_for_query
                    live_bullets = get_rag_bullets_for_query(query)
                
                if live_bullets:
                    rag_bullets.extend(live_bullets)
                else:
                    # æ·»åŠ é™çº§æç¤º
                    rag_bullets.append(f"æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•è·å–'{query}'çš„æœ€æ–°ä¿¡æ¯ï¼Œè¯·ç¨åå†è¯•æˆ–å°è¯•å…¶ä»–æŸ¥è¯¢ã€‚")
                    
        except Exception as e:
            logging.warning("å®æ—¶æœç´¢RAGå¤±è´¥ï¼Œè·³è¿‡ï¼š%s", e)
    
    # â€”â€” æ–°å¢ï¼šå³ä½¿ä¸éœ€è¦æ–°æœç´¢ï¼Œä¹Ÿè¦ä¼ é€’å·²æœç´¢çš„å†…å®¹ â€”â€” #
    elif session_id:
        try:
            from llm.search_cache_manager import get_session_searched_content
            searched_content = get_session_searched_content(session_id)
            if searched_content:
                # å°†å·²æœç´¢çš„å†…å®¹æ·»åŠ åˆ°rag_bulletsä¸­
                rag_bullets.append(f"å·²æœç´¢çš„ç›¸å…³ä¿¡æ¯ï¼š\n{searched_content}")
        except Exception as e:
            logging.warning(f"[æœç´¢ä¼˜åŒ–] æ·»åŠ å·²æœç´¢å†…å®¹å¤±è´¥: {e}")

    # â€”â€” æ‹¼è£…æœ€ç»ˆ Prompt â€”â€” #
    final_prompt = build_final_prompt(
        {**analysis, "rag_bullets": rag_bullets, "rag_queries": analysis.get("rag_queries", [])},
        state_summary,
        question,
        current_time,
        user_memories,  # æ–°å¢ï¼šä¼ é€’ç”¨æˆ·è®°å¿†ç‚¹
        user_info  # æ–°å¢ï¼šä¼ é€’ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
    )
    
    # æ ¼å¼åŒ–æ˜¾ç¤ºæœ€ç»ˆprompt
    logging.info("=" * 50)
    logging.info("ğŸ¯ æœ€ç»ˆæ‹¼æ¥çš„ PROMPT")
    logging.info("=" * 50)
    logging.info(final_prompt)
    logging.info("=" * 50)

    # â€”â€” ç”Ÿæˆ â€”â€” #
    resp = chat_with_llm(final_prompt)
    answer = resp.get("answer", "") if isinstance(resp, dict) else resp
    
    # æ¸…ç†å¯èƒ½å‡ºç°çš„å¤šä½™å¼•å·
    if isinstance(answer, str):
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†æ‰€æœ‰ç±»å‹çš„å¼•å·ï¼ˆåŒ…æ‹¬Unicodeå¼•å·ï¼‰
        import re
        # ç§»é™¤æ‰€æœ‰ç±»å‹çš„å¼•å·ï¼ˆåŒ…æ‹¬Unicodeå¼•å·ï¼‰
        answer = re.sub(r'^["""''""]+', '', answer)  # ç§»é™¤å¼€å¤´çš„å¼•å·
        answer = re.sub(r'["""''""]+$', '', answer)  # ç§»é™¤ç»“å°¾çš„å¼•å·
        answer = answer.strip()  # ç§»é™¤ç©ºç™½å­—ç¬¦

    # â€”â€” å¤±è´¥å›é€€ï¼ˆæ ¹æ® emotion_type é€‚é…ï¼‰â€”â€” #
    if not isinstance(answer, str) or len(answer.strip()) < 4:
        emotion_type = analysis.get("emotion_type", "neutral")
        fallback = {
            "tired": "æˆ‘åœ¨ï¼Œå…ˆä¼‘æ¯ä¸€ä¸‹ï¼Œç­‰ä½ æƒ³è¯´çš„æ—¶å€™æˆ‘ä»¬å†èŠã€‚",
            "negative": "æˆ‘ç†è§£ä½ çš„æ„Ÿå—ï¼Œå…ˆè®©æƒ…ç»ªæ²‰æ·€ä¸€ä¸‹ï¼Œæˆ‘åœ¨è¿™é‡Œé™ªç€ä½ ã€‚",
            "angry": "æˆ‘å¬è§ä½ çš„æ„¤æ€’äº†ï¼Œå…ˆå†·é™ä¸€ä¸‹ï¼Œæˆ‘æ”¯æŒä½ ã€‚",
            "positive": "çœŸä¸ºä½ å¼€å¿ƒï¼æƒ³ç»§ç»­åˆ†äº«è¿™ä»½å–œæ‚¦å—ï¼Ÿ",
            "neutral": "æˆ‘åœ¨ï¼Œæƒ³èŠä»€ä¹ˆéƒ½å¯ä»¥ã€‚"
        }
        answer = fallback.get(emotion_type, fallback["neutral"])

    return answer