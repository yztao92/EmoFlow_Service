# File: prompts/prompt_flow_controller.py
import logging
from typing import List, Dict, Any
from prompts.chat_prompts_generator import build_conversation_messages
from llm.llm_factory import chat_with_llm, chat_with_llm_messages

try:
    from retriever.search import retrieve
except Exception:
    logging.warning("[prompt_flow] æœªæ‰¾åˆ° retriever.search.retrieveï¼ŒRAG å°†è¢«ç¦ç”¨")
    def retrieve(queries: List[str], top_k: int = 4):
        return []

def chat_once(analysis: dict, state_summary: str, question: str, current_time: str = None, user_id: int = None, user_info: Dict[str, Any] = None, session_id: str = None, conversation_history: List[Dict[str, str]] = None) -> str:
    # â€”â€” è·å–ç”¨æˆ·è®°å¿†ç‚¹ï¼ˆå¦‚æœæœ‰user_idï¼‰â€”â€” #
    user_memories = []
    if user_id:
        try:
            from memory import get_user_latest_memories
            user_memories = get_user_latest_memories(user_id, limit=5)
            if user_memories:
                logging.info(f"ğŸ“ è·å–åˆ°ç”¨æˆ· {user_id} çš„ {len(user_memories)} ä¸ªè®°å¿†ç‚¹")
            else:
                logging.info(f"ğŸ“ ç”¨æˆ· {user_id} æš‚æ— è®°å¿†ç‚¹")
        except Exception as e:
            logging.warning(f"è·å–ç”¨æˆ·è®°å¿†ç‚¹å¤±è´¥ï¼Œè·³è¿‡ï¼š{e}")
            user_memories = []

    # â€”â€” å¯é€‰ RAG â€”â€” #
    rag_bullets = []
    if analysis.get("need_rag"):
        try:
            docs = retrieve(analysis.get("rag_queries", []), top_k=4)
            rag_bullets = [getattr(d, "snippet", str(d)) for d in (docs or [])]
        except Exception as e:
            logging.warning("RAG æ£€ç´¢å¤±è´¥ï¼Œè·³è¿‡ï¼š%s", e)

    # â€”â€” å®æ—¶æœç´¢RAGï¼ˆä¼˜å…ˆä½¿ç”¨ç¼“å­˜ï¼Œå¿…è¦æ—¶è¿›è¡Œæ–°æœç´¢ï¼‰â€”â€” #
    if analysis.get("need_live_search"):
        try:
            from llm.qwen_live_search import search_live_multiple
            
            live_queries = analysis.get("live_search_queries", [])
            has_timeliness_requirement = analysis.get("has_timeliness_requirement", False)
            logging.info(f"[å®æ—¶æœç´¢] å¼€å§‹å¤„ç† {len(live_queries)} ä¸ªæœç´¢æŸ¥è¯¢")
            logging.info(f"[å®æ—¶æœç´¢] æ—¶æ•ˆæ€§è¦æ±‚: {has_timeliness_requirement}")
            
            # ä½¿ç”¨ç‹¬ç«‹çš„åƒé—®å®æ—¶æ£€ç´¢æ¨¡å—
            live_results = search_live_multiple(live_queries, has_timeliness_requirement, session_id=session_id)
            
            if live_results:
                rag_bullets.extend(live_results)
                logging.info(f"[å®æ—¶æœç´¢] è·å¾— {len(live_results)} ä¸ªæœç´¢ç»“æœ")
            else:
                logging.warning("[å®æ—¶æœç´¢] æœªè·å¾—ä»»ä½•æœç´¢ç»“æœ")
                    
        except Exception as e:
            logging.warning("å®æ—¶æœç´¢RAGå¤±è´¥ï¼Œè·³è¿‡ï¼š%s", e)
    
    # â€”â€” å¦‚æœæ²¡æœ‰æ–°æœç´¢ç»“æœï¼Œå°è¯•ä»ç¼“å­˜ä¸­è·å–å·²æœ‰çš„æœç´¢ä¿¡æ¯ â€”â€” #
    if not rag_bullets and session_id:
        try:
            from llm.search_cache import get_cached_search_results
            cached_results = get_cached_search_results(session_id)
            if cached_results:
                # å–æœ€è¿‘3æ¡ç¼“å­˜ç»“æœ
                for result in cached_results[-3:]:
                    rag_bullets.append(result['result'])
                logging.info(f"[ç¼“å­˜æœç´¢] å·²åŠ è½½ {len(cached_results)} æ¡ç¼“å­˜æœç´¢ä¿¡æ¯åˆ°å‚è€ƒçŸ¥è¯†")
        except Exception as e:
            logging.warning(f"[ç¼“å­˜æœç´¢] è·å–ç¼“å­˜æœç´¢ä¿¡æ¯å¤±è´¥: {e}")

    # â€”â€” æ‹¼è£…å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ â€”â€” #
    messages = build_conversation_messages(
        {**analysis, "rag_bullets": rag_bullets, "rag_queries": analysis.get("rag_queries", [])},
        question,
        current_time,
        user_memories,  # ä¼ é€’ç”¨æˆ·è®°å¿†ç‚¹
        user_info,  # ä¼ é€’ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        conversation_history  # ä¼ é€’å¯¹è¯å†å²
    )
    
    # æ ¼å¼åŒ–æ˜¾ç¤ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆå·²ç¦ç”¨ï¼‰
    # logging.info("=" * 50)
    # logging.info("ğŸ¯ æœ€ç»ˆæ‹¼æ¥çš„æ¶ˆæ¯åˆ—è¡¨")
    # logging.info("=" * 50)
    # for i, msg in enumerate(messages):
    #     logging.info(f"æ¶ˆæ¯ {i+1} [{msg['role']}]: {msg['content'][:100]}...")
    # logging.info("=" * 50)

    # â€”â€” ç”Ÿæˆ â€”â€” #
    resp = chat_with_llm_messages(messages)
    answer = resp.get("answer", "") if isinstance(resp, dict) else resp
    
    # æ¸…ç†å¯èƒ½å‡ºç°çš„å¤šä½™å¼•å·
    if isinstance(answer, str):
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ˆå·²ç¦ç”¨ï¼‰
        # logging.info(f"ğŸ” å¼•å·æ¸…ç†å‰: '{answer}'")
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†æ‰€æœ‰ç±»å‹çš„å¼•å·ï¼ˆåŒ…æ‹¬Unicodeå¼•å·ï¼‰
        import re
        # ç§»é™¤æ‰€æœ‰ç±»å‹çš„å¼•å·ï¼ˆåŒ…æ‹¬Unicodeå¼•å·ï¼‰
        answer = re.sub(r'^["""''""]+', '', answer)  # ç§»é™¤å¼€å¤´çš„å¼•å·
        answer = re.sub(r'["""''""]+$', '', answer)  # ç§»é™¤ç»“å°¾çš„å¼•å·
        answer = answer.strip()  # ç§»é™¤ç©ºç™½å­—ç¬¦
        
        # logging.info(f"ğŸ” å¼•å·æ¸…ç†å: '{answer}'")
    
    # æ ¼å¼åŒ–æ˜¾ç¤ºLLMè¿”å›ç»“æœï¼ˆå·²ç¦ç”¨ï¼‰
    # logging.info("=" * 50)
    # logging.info("ğŸ¤– LLM è¿”å›ç»“æœ")
    # logging.info("=" * 50)
    # logging.info(f"åŸå§‹å“åº”: {resp}")
    # logging.info(f"æå–ç­”æ¡ˆ: {answer}")
    # logging.info("=" * 50)

    # â€”â€” å¤±è´¥å›é€€ï¼ˆæ ¹æ® emotion_type é€‚é…ï¼‰â€”â€” #
    if not isinstance(answer, str) or len(answer.strip()) < 4:
        emotion_type = analysis.get("emotion_type", "neutral")
        fallback = {
            "tired": "æˆ‘åœ¨ï¼Œå…ˆä¼‘æ¯ä¸€ä¸‹ï¼Œç­‰ä½ æƒ³è¯´çš„æ—¶å€™æˆ‘ä»¬å†èŠã€‚",
            "negative": "æˆ‘ç†è§£ä½ çš„æ„Ÿå—ï¼Œå…ˆè®©æƒ…ç»ªæ²‰æ·€ä¸€ä¸‹ï¼Œæˆ‘åœ¨è¿™é‡Œé™ªç€ä½ ã€‚",
            "angry": "æˆ‘å¬è§ä½ çš„æ„¤æ€’äº†ï¼Œå…ˆå†·é™ä¸€ä¸‹ï¼Œæˆ‘æ”¯æŒä½ ã€‚",
            "positive": "çœŸä¸ºä½ å¼€å¿ƒï¼æƒ³ç»§ç»­åˆ†äº«è¿™ä»½å–œæ‚¦å—ï¼Ÿ",
            "neutral": "æˆ‘åœ¨ï¼Œæƒ³èŠä»€ä¹ˆéƒ½å¯ä»¥ã€‚"
        }
        answer = fallback.get(emotion_type, fallback["neutral"])

    return answer