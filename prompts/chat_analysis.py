# prompts/chat_analysis.py
import json
import logging
from typing import Dict, Any
from llm.llm_factory import chat_with_llm

ANALYZE_PROMPT = """
ä½ æ˜¯å¯¹è¯åˆ†æå™¨ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·å¯¹è¯å†…å®¹å¹¶è¾“å‡ºç»“æ„åŒ–åˆ†æç»“æœã€‚

## åˆ†æå­—æ®µè¯´æ˜

### emotion (ç”¨æˆ·çš„æƒ…ç»ª)
- "ä½è°·": ç”¨æˆ·è¡¨è¾¾æ‚²ä¼¤ã€ç—›è‹¦ã€ç»æœ›ã€æŠ‘éƒç­‰è´Ÿé¢æƒ…ç»ª
- "æ™®é€š": ç”¨æˆ·æƒ…ç»ªå¹³å’Œï¼Œæ­£å¸¸èŠå¤©çŠ¶æ€
- "åº†ç¥": ç”¨æˆ·è¡¨è¾¾å¼€å¿ƒã€å…´å¥‹ã€æ»¡è¶³ã€å¹¸ç¦ç­‰æ­£é¢æƒ…ç»ª

### stage (å¯¹è¯é˜¶æ®µåˆ†æ)
- "æš–åœº": å¯¹è¯åˆšå¼€å§‹ï¼Œéœ€è¦å»ºç«‹æ°›å›´å’Œä¿¡ä»»
- "å»ºè®®": å¯¹è¯ä¸­æœŸï¼Œå¯ä»¥ç»™å‡ºå»ºè®¾æ€§å»ºè®®
- "æ”¶å°¾": å¯¹è¯åæœŸï¼Œè‡ªç„¶æ”¶æŸï¼Œé¿å…è¿‡åº¦æ·±å…¥

### context_type (å¯¹è¯ç±»å‹)
- "æ±‚å»ºè®®": ç”¨æˆ·æ˜ç¡®éœ€è¦è§£å†³æ–¹æ¡ˆæˆ–å»ºè®®
- "æ±‚å®‰æ…°": ç”¨æˆ·å¯»æ±‚æƒ…æ„Ÿæ”¯æŒå’Œç†è§£
- "é—²èŠ": æ—¥å¸¸èŠå¤©ï¼Œæ— ç‰¹æ®Šéœ€æ±‚
- "ç©æ¢—": ç”¨æˆ·å¼€ç©ç¬‘æˆ–ä½¿ç”¨å¹½é»˜è¡¨è¾¾
- "å…¶ä»–": ä¸å±äºä»¥ä¸Šç±»å‹çš„å¯¹è¯

### ask_slot (æé—®ç­–ç•¥)
- "gentle": æ¸©å’Œå¼•å¯¼ï¼Œç”¨å…±é¸£ã€åˆ†äº«æˆ–è½»ææ·¡å†™çš„æ–¹å¼å¼•å¯¼ç”¨æˆ·è‡ªç„¶è¯´å‡ºæ›´å¤šï¼Œé¿å…ç›´æ¥æé—®
- "active": ä¸»åŠ¨æé—®ï¼Œç›´æ¥è¯¢é—®ç”¨æˆ·ç›¸å…³ç»†èŠ‚æˆ–æ„Ÿå—


### need_empathy (æ˜¯å¦éœ€è¦å…±æƒ…)
- true: ç”¨æˆ·æƒ…ç»ªä½è½ã€å¯»æ±‚å®‰æ…°ã€è¡¨è¾¾ç—›è‹¦ã€éœ€è¦æƒ…æ„Ÿæ”¯æŒæ—¶
- false: ç”¨æˆ·æƒ…ç»ªå¹³å’Œã€é—²èŠã€åˆ†äº«æ—¥å¸¸ã€æƒ…ç»ªç¨³å®šæ—¶

### need_rag (æ˜¯å¦éœ€è¦çŸ¥è¯†æ£€ç´¢)
- true: ç”¨æˆ·é—®é¢˜éœ€è¦ä¸“ä¸šçŸ¥è¯†æˆ–å»ºè®®æ”¯æŒ
- false: çº¯æƒ…æ„Ÿäº¤æµï¼Œæ— éœ€é¢å¤–çŸ¥è¯†

### queries (æ£€ç´¢æŸ¥è¯¢çŸ­è¯­)
- å½“need_rag=trueæ—¶ï¼Œæä¾›2-4ä¸ªæ£€ç´¢å…³é”®è¯
- æ¯ä¸ªçŸ­è¯­6-12å­—ï¼Œç”¨äºçŸ¥è¯†åº“æ£€ç´¢

## è¾“å‡ºè¦æ±‚
ä»…è¾“å‡ºJSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚

## åˆ†æè¾“å…¥
è½®æ¬¡: {round_index}
å†å²å¯¹è¯: {state_summary}
ç”¨æˆ·è¾“å…¥: {question}

ä¸Šè½®æ˜¯å¦å·²æé—®: {last_turn_had_question}
"""

def _normalize_queries(qlist):
    """æŠŠ queries ç»Ÿä¸€ä¸º [{'q': str, 'weight': 1.0}, ...]"""
    if not isinstance(qlist, list):
        return []
    out = []
    for q in qlist:
        if isinstance(q, str):
            qs = q.strip()
            if qs:
                out.append({"q": qs, "weight": 1.0})
        elif isinstance(q, dict) and isinstance(q.get("q"), str):
            qs = q["q"].strip()
            if qs:
                w = q.get("weight", 1.0)
                try:
                    w = float(w)
                except Exception:
                    w = 1.0
                out.append({"q": qs, "weight": w})
        elif isinstance(q, dict) and isinstance(q.get("q"), dict):
            # å¦‚æœq["q"]ä¹Ÿæ˜¯å­—å…¸ï¼Œè·³è¿‡è¿™ä¸ªå…ƒç´ 
            logging.warning(f"ğŸ§  [åˆ†æ] è·³è¿‡åµŒå¥—å­—å…¸æŸ¥è¯¢: {q}")
            continue
    return out

def analyze_turn(
    round_index:int,
    state_summary:str,
    question:str,
    last_turn_had_question:str="no"
) -> Dict[str, Any]:

    prompt = ANALYZE_PROMPT.format(
        round_index=round_index,
        state_summary=state_summary,
        question=question,
        last_turn_had_question=last_turn_had_question
    )

    logging.info("ğŸ§  [åˆ†æ] å…¥å‚ â†’ round=%s, question=%s", round_index, question)
    logging.info("ğŸ§  [åˆ†æ] Prompt â†“\n%s", prompt)

    res = chat_with_llm(prompt)  # çº¦å®šè¿”å› {"answer": "..."}
    raw_output = res.get("answer", "")

    # é»˜è®¤å…œåº•ç»“æ„
    parsed = {
        "mode": "æ™®é€š",
        "stage": "æš–åœº",
        "context_type": "é—²èŠ",
        "ask_slot": "gentle",  # é»˜è®¤ä½¿ç”¨æ¸©å’Œå¼•å¯¼ï¼Œé¿å…ç›´æ¥æé—®
        "need_empathy": False,
        "need_rag": False,
        "queries": []
    }

    # è§£æ LLM JSONï¼ˆå¤±è´¥å°±ç”¨å…œåº•ï¼‰
    try:
        parsed.update(json.loads(raw_output))
    except Exception as e:
        logging.error("ğŸ§  [åˆ†æ] JSON è§£æå¤±è´¥ â†’ %s", e, exc_info=True)

    # è§„èŒƒåŒ– queries
    parsed["queries"] = _normalize_queries(parsed.get("queries", []))

    logging.info("ğŸ§  [åˆ†æ] ç»“æ„åŒ–ç»“æœ â†“\n%s", json.dumps(parsed, ensure_ascii=False, indent=2))
    return parsed