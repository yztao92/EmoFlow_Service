import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))  # åŠ å…¥é¡¹ç›®æ ¹ç›®å½•

from dotenv import load_dotenv, find_dotenv
from langchain_community.document_loaders import TextLoader
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from llm.zhipu_embedding import ZhipuEmbedding

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv(find_dotenv())

# è®¾ç½®ç›®å½•
data_dir = "/root/EmoFlow/data"
save_base_path = "/root/EmoFlow/data/vectorstore"
os.makedirs(save_base_path, exist_ok=True)

# æ¸…æ´—æ–‡æœ¬å‡½æ•°
def clean_text(text: str) -> str:
    skip_keywords = ["ç›®å½•", "å‰è¨€", "åºè¨€", "è‡´è°¢", "ç‰ˆæƒ", "é™„å½•", "Contents", "Preface", "Prologue"]
    lines = text.splitlines()
    cleaned = []
    skipping = False

    for line in lines:
        line = line.strip()
        if not line:  # å»é™¤ç©ºè¡Œ
            continue

        # æ£€æµ‹è·³è¿‡å¼€å§‹
        if any(kw in line for kw in skip_keywords):
            skipping = True
            continue

        # æ£€æµ‹è·³è¿‡ç»“æŸï¼ˆé‡åˆ°æ­£æ–‡ç‰¹å¾ï¼‰
        if skipping and (len(line) > 30 or line.startswith("ç¬¬") or line[0].isdigit()):
            skipping = False

        if not skipping:
            cleaned.append(line)

    return "\n".join(cleaned)

# åˆå§‹åŒ– embedding æ¨¡å‹
embedding = ZhipuEmbedding()

# å¤„ç†æ¯ä¸ª txt æ–‡ä»¶
txt_files = [f for f in os.listdir(data_dir) if f.endswith(".txt")]
for filename in txt_files:
    name = os.path.splitext(filename)[0]
    file_path = os.path.join(data_dir, filename)

    print(f"ğŸ“˜ æ­£åœ¨å¤„ç†: {filename}")

    # åŠ è½½å¹¶æ¸…æ´—æ–‡æœ¬
    with open(file_path, "r", encoding="utf-8") as f:
        raw_text = f.read()
    cleaned_text = clean_text(raw_text)
    docs = [Document(page_content=cleaned_text)]

    # åˆ‡å‰²æ–‡æœ¬
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = splitter.split_documents(docs)

    # æ„å»ºå‘é‡åº“
    vectorstore = FAISS.from_documents(split_docs, embedding)

    # ä¿å­˜å‘é‡åº“
    save_path = os.path.join(save_base_path, name)
    os.makedirs(save_path, exist_ok=True)
    vectorstore.save_local(save_path)

    print(f"âœ… æ„å»ºå®Œæˆ: {name}ï¼Œæ®µæ•°: {len(split_docs)}")
    print(f"ğŸ“‚ ä¿å­˜è·¯å¾„: {save_path}\n")